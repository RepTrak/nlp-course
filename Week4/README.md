# Week 4
## Seq2Seq and Attention

During this lesson we will cover the next important NLP task - Seq2Seq. We will see how LSTM/RNN shine in solving this task and discuss how Seq2Seq relates to Language modeling (spoiler alert: one is a subset of another). Also we will talk about attention - the single change that began a revolution in NLP world
### Link to seminar #4 in google colab:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RepTrak/nlp-course/blob/develop/Week4/seminar_4.ipynb)

### Link to homework #4 in google colab:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RepTrak/nlp-course/blob/develop/Week4/homework_4.ipynb)
